{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia_detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0NKn0V6oMRiF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Kaggle Competition Stage 1: Submission 1 - CNN"
      ]
    },
    {
      "metadata": {
        "id": "icalNsTjZRy9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dmky-6UpeqYu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Connect GoogleDrive"
      ]
    },
    {
      "metadata": {
        "id": "RSGHDdOAjhVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# install PyDrive, only run one time\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Get authorize, only need in first time you run the code\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aqMmpysUjhji",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Looking for ID\n",
        "file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "file_list = drive.ListFile({'q': \"'1p3moyb19csZ3O5aQgCX2jT84ISFL3Cd0' in parents and trashed=false\"}).GetList()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aBIbLMGyjhtg",
        "colab_type": "code",
        "outputId": "de587fda-99b3-46a7-99c5-7b083ebf57cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Google Drive File ID\n",
        "listed = drive.ListFile({'q': \"title contains 'stage_1_train_labels.csv' and '1p3moyb19csZ3O5aQgCX2jT84ISFL3Cd0' in parents\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))\n",
        "  \n",
        "listed = drive.ListFile({'q': \"title contains 'stage_1_train_images.zip' and '1p3moyb19csZ3O5aQgCX2jT84ISFL3Cd0' in parents\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))\n",
        "  \n",
        "listed = drive.ListFile({'q': \"title contains 'stage_1_test_images.zip' and '1p3moyb19csZ3O5aQgCX2jT84ISFL3Cd0' in parents\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title stage_1_train_labels.csv, id 1n1EnK1Jq5fe-3rY5t7u0Fe8LvAJPaeW0\n",
            "title stage_1_train_images.zip, id 1cZtHGtv1S4rw0DBMmGW6AmA0v7Ypwe5V\n",
            "title stage_1_test_images.zip, id 1j2UYG2sVPReaPhHGZ6sQkzi7wSOAXtG4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QKQTaW4Djh1U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Files\n",
        "train_zip = drive.CreateFile({'id': '1cZtHGtv1S4rw0DBMmGW6AmA0v7Ypwe5V'}) \n",
        "test_zip = drive.CreateFile({'id': '1j2UYG2sVPReaPhHGZ6sQkzi7wSOAXtG4'}) \n",
        "train_zip.GetContentFile('stage_1_train_images.zip', \"zip\")\n",
        "test_zip.GetContentFile('stage_1_test_images.zip', \"zip\")\n",
        "train_labels = drive.CreateFile({'id': '1n1EnK1Jq5fe-3rY5t7u0Fe8LvAJPaeW0'})\n",
        "train_labels.GetContentFile('stage_1_train_labels.csv', \"csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8zMh545Bjh6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://blog.csdn.net/ice110956/article/details/26597179\n",
        "# http://blog.51cto.com/wangwei007/1045577\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def un_zip(file_name):\n",
        "    \"\"\"unzip zip file\"\"\"\n",
        "    zip_file = zipfile.ZipFile(file_name)\n",
        "    if os.path.isdir(file_name + \"_files\"):\n",
        "        pass\n",
        "    else:\n",
        "        os.mkdir(file_name + \"_files\")\n",
        "    for names in zip_file.namelist():\n",
        "        zip_file.extract(names,file_name + \"_files/\")\n",
        "    zip_file.close()\n",
        "\n",
        "un_zip('stage_1_train_images.zip')\n",
        "un_zip('stage_1_test_images.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YJs6BJuYlKaM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN Model\n",
        "\n",
        "\n",
        "This follow the kaggle tutorial:\n",
        " https://www.kaggle.com/jonnedtc/cnn-segmentation-connected-components \n",
        "\n",
        "Approach\n",
        "1.   Firstly a convolutional neural network is used to segment the image, using the bounding boxes directly as a mask.\n",
        "2.   Secondly connected components is used to separate multiple areas of predicted pneumonia.\n",
        "3.   Finally a bounding box is simply drawn around every connected component.\n",
        "\n",
        "Network\n",
        "*    The network consists of a number of residual blocks with convolutions and downsampling blocks with max pooling.\n",
        "*    At the end of the network a single upsampling layer converts the output to the same shape as the input.\n",
        "\n",
        "\n",
        "As the input to the network is 256 by 256 (instead of the original 1024 by 1024) and the network downsamples a number of times without any meaningful upsampling (the final upsampling)"
      ]
    },
    {
      "metadata": {
        "id": "sljcpgILDbes",
        "colab_type": "code",
        "outputId": "13407751-0d20-4437-93c0-7159c543a976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Install and Import\n",
        "!pip install pydicom # manually install pydicom to Google Colab \n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import pydicom\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "from skimage import measure\n",
        "from skimage.transform import resize\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "texRS4Ojc4cA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "download_path = os.path.expanduser('~/data')\n",
        "try:\n",
        "  os.makedirs(download_path)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "from zipfile import ZipFile\n",
        "train_images = ZipFile('stage_1_train_images.zip', 'r')\n",
        "\n",
        "#train_images.extractall(os.path.join(download_path, 'stage_1_train_images.zip'))\n",
        "#train_images.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qEjeNP3MF8G_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# empty dictionary\n",
        "pneumonia_locations = {}\n",
        "\n",
        "# load table\n",
        "with open('stage_1_train_labels.csv', mode='r') as infile:\n",
        "    # open reader\n",
        "    reader = csv.reader(infile)\n",
        "    # skip header\n",
        "    next(reader, None)\n",
        "    # loop through rows\n",
        "    for rows in reader:\n",
        "        # retrieve information\n",
        "        filename = rows[0]\n",
        "        location = rows[1:5]\n",
        "        pneumonia = rows[5]\n",
        "        # if row contains pneumonia add label to dictionary\n",
        "        # which contains a list of pneumonia locations per filename\n",
        "        if pneumonia == '1':\n",
        "            # convert string to float to int\n",
        "            location = [int(float(i)) for i in location]\n",
        "            # save pneumonia location in dictionary\n",
        "            if filename in pneumonia_locations:\n",
        "                pneumonia_locations[filename].append(location)\n",
        "            else:\n",
        "                pneumonia_locations[filename] = [location]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "anFF91AsGCd4",
        "colab_type": "code",
        "outputId": "86872af9-f0d4-4fec-f2e8-f51d1ff8a248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# load and shuffle filenames\n",
        "folder = './stage_1_train_images.zip_files'\n",
        "filenames = os.listdir(folder)\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# split into train and validation filenames\n",
        "n_valid_samples = 2560\n",
        "train_filenames = filenames[n_valid_samples:]\n",
        "valid_filenames = filenames[:n_valid_samples]\n",
        "print('n train samples', len(train_filenames))\n",
        "print('n valid samples', len(valid_filenames))\n",
        "n_train_samples = len(filenames) - n_valid_samples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n train samples 23124\n",
            "n valid samples 2560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v67zalxzQpQw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Generator\n",
        "\n",
        "The dataset is too large to fit into memory, so we need to create a generator that loads data on the fly.\n",
        "\n",
        "\n",
        "*   The generator takes in some filenames, batch_size and other parameters.\n",
        "*   The generator outputs a random batch of numpy images and numpy masks.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3weODEETGFzZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class generator(keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, folder, filenames, pneumonia_locations=None, batch_size=32, image_size=256, shuffle=True, augment=False, predict=False):\n",
        "        self.folder = folder\n",
        "        self.filenames = filenames\n",
        "        self.pneumonia_locations = pneumonia_locations\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "        self.predict = predict\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __load__(self, filename):\n",
        "        # load dicom file as numpy array\n",
        "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
        "        # create empty mask\n",
        "        msk = np.zeros(img.shape)\n",
        "        # get filename without extension\n",
        "        filename = filename.split('.')[0]\n",
        "        # if image contains pneumonia\n",
        "        if filename in self.pneumonia_locations:\n",
        "            # loop through pneumonia\n",
        "            for location in self.pneumonia_locations[filename]:\n",
        "                # add 1's at the location of the pneumonia\n",
        "                x, y, w, h = location\n",
        "                msk[y:y+h, x:x+w] = 1\n",
        "        # resize both image and mask\n",
        "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
        "        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n",
        "        # if augment then horizontal flip half the time\n",
        "        if self.augment and random.random() > 0.5:\n",
        "            img = np.fliplr(img)\n",
        "            msk = np.fliplr(msk)\n",
        "        # add trailing channel dimension\n",
        "        img = np.expand_dims(img, -1)\n",
        "        msk = np.expand_dims(msk, -1)\n",
        "        return img, msk\n",
        "    \n",
        "    def __loadpredict__(self, filename):\n",
        "        # load dicom file as numpy array\n",
        "        img = pydicom.dcmread(os.path.join(self.folder, filename)).pixel_array\n",
        "        # resize image\n",
        "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
        "        # add trailing channel dimension\n",
        "        img = np.expand_dims(img, -1)\n",
        "        return img\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # select batch\n",
        "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # predict mode: return images and filenames\n",
        "        if self.predict:\n",
        "            # load files\n",
        "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
        "            # create numpy batch\n",
        "            imgs = np.array(imgs)\n",
        "            return imgs, filenames\n",
        "        # train mode: return images and masks\n",
        "        else:\n",
        "            # load files\n",
        "            items = [self.__load__(filename) for filename in filenames]\n",
        "            # unzip images and masks\n",
        "            imgs, msks = zip(*items)\n",
        "            # create numpy batch\n",
        "            imgs = np.array(imgs)\n",
        "            msks = np.array(msks)\n",
        "            return imgs, msks\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.filenames)\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.predict:\n",
        "            # return everything\n",
        "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
        "        else:\n",
        "            # return full batches only\n",
        "            return int(len(self.filenames) / self.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P0tskMiSGWGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Network\n",
        "\n",
        "\n",
        "def create_downsample(channels, inputs):\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 1, padding='same', use_bias=False)(x)\n",
        "    x = keras.layers.MaxPool2D(2)(x)\n",
        "    return x\n",
        "\n",
        "def create_resblock(channels, inputs):\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(inputs)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(x)\n",
        "    return keras.layers.add([x, inputs])\n",
        "\n",
        "def create_network(input_size, channels, n_blocks=2, depth=4):\n",
        "    # input\n",
        "    inputs = keras.Input(shape=(input_size, input_size, 1))\n",
        "    x = keras.layers.Conv2D(channels, 3, padding='same', use_bias=False)(inputs)\n",
        "    # residual blocks\n",
        "    for d in range(depth):\n",
        "        channels = channels * 2\n",
        "        x = create_downsample(channels, x)\n",
        "        for b in range(n_blocks):\n",
        "            x = create_resblock(channels, x)\n",
        "    # output\n",
        "    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n",
        "    x = keras.layers.LeakyReLU(0)(x)\n",
        "    x = keras.layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
        "    outputs = keras.layers.UpSampling2D(2**depth)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FI1CtuWdGYK0",
        "colab_type": "code",
        "outputId": "480eb610-a7fb-4ed9-b8bc-259df8173af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        }
      },
      "cell_type": "code",
      "source": [
        "# define iou or jaccard loss function\n",
        "def iou_loss(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n",
        "    return 1 - score\n",
        "\n",
        "# combine bce loss and iou loss\n",
        "def iou_bce_loss(y_true, y_pred):\n",
        "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)\n",
        "\n",
        "# mean iou as a metric\n",
        "def mean_iou(y_true, y_pred):\n",
        "    y_pred = tf.round(y_pred)\n",
        "    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
        "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
        "    smooth = tf.ones(tf.shape(intersect))\n",
        "    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n",
        "\n",
        "# create network and compiler\n",
        "model = create_network(input_size=256, channels=32, n_blocks=2, depth=4)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=iou_bce_loss,\n",
        "              metrics=['accuracy', mean_iou])\n",
        "\n",
        "# cosine learning rate annealing\n",
        "def cosine_annealing(x):\n",
        "    lr = 0.001\n",
        "    epochs = 25\n",
        "    return lr*(np.cos(np.pi*x/epochs)+1.)/2\n",
        "learning_rate = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)\n",
        "\n",
        "# create train and validation generators\n",
        "folder = './stage_1_train_images.zip_files'\n",
        "train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)\n",
        "valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=False, predict=False)\n",
        "\n",
        "history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=[learning_rate], epochs=25, workers=4, use_multiprocessing=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "722/722 [==============================] - 1659s 2s/step - loss: 15.6339 - acc: 0.9621 - mean_iou: 0.6169 - val_loss: 15.6428 - val_acc: 0.9639 - val_mean_iou: 0.6197\n",
            "Epoch 2/25\n",
            "722/722 [==============================] - 1080s 1s/step - loss: 14.4527 - acc: 0.9665 - mean_iou: 0.6697 - val_loss: 14.3044 - val_acc: 0.9610 - val_mean_iou: 0.6323\n",
            "Epoch 3/25\n",
            "722/722 [==============================] - 1075s 1s/step - loss: 13.8554 - acc: 0.9682 - mean_iou: 0.6860 - val_loss: 15.3587 - val_acc: 0.9384 - val_mean_iou: 0.5188\n",
            "Epoch 4/25\n",
            "722/722 [==============================] - 1082s 1s/step - loss: 13.5442 - acc: 0.9696 - mean_iou: 0.7011 - val_loss: 14.4012 - val_acc: 0.9714 - val_mean_iou: 0.7429\n",
            "Epoch 5/25\n",
            "722/722 [==============================] - 1089s 2s/step - loss: 13.3252 - acc: 0.9705 - mean_iou: 0.7087 - val_loss: 17.3704 - val_acc: 0.9724 - val_mean_iou: 0.7762\n",
            "Epoch 6/25\n",
            "722/722 [==============================] - 1083s 1s/step - loss: 13.0937 - acc: 0.9711 - mean_iou: 0.7169 - val_loss: 13.2745 - val_acc: 0.9642 - val_mean_iou: 0.6649\n",
            "Epoch 7/25\n",
            "722/722 [==============================] - 1081s 1s/step - loss: 12.9467 - acc: 0.9713 - mean_iou: 0.7165 - val_loss: 13.9623 - val_acc: 0.9739 - val_mean_iou: 0.7448\n",
            "Epoch 8/25\n",
            "722/722 [==============================] - 1081s 1s/step - loss: 12.8163 - acc: 0.9719 - mean_iou: 0.7203 - val_loss: 12.9758 - val_acc: 0.9716 - val_mean_iou: 0.7264\n",
            "Epoch 9/25\n",
            "722/722 [==============================] - 1083s 1s/step - loss: 12.7388 - acc: 0.9722 - mean_iou: 0.7251 - val_loss: 12.8740 - val_acc: 0.9728 - val_mean_iou: 0.7421\n",
            "Epoch 10/25\n",
            "722/722 [==============================] - 1084s 2s/step - loss: 12.6022 - acc: 0.9728 - mean_iou: 0.7284 - val_loss: 12.9177 - val_acc: 0.9737 - val_mean_iou: 0.7489\n",
            "Epoch 11/25\n",
            "722/722 [==============================] - 1079s 1s/step - loss: 12.5286 - acc: 0.9730 - mean_iou: 0.7343 - val_loss: 12.7609 - val_acc: 0.9732 - val_mean_iou: 0.7256\n",
            "Epoch 12/25\n",
            "722/722 [==============================] - 1093s 2s/step - loss: 12.4618 - acc: 0.9733 - mean_iou: 0.7342 - val_loss: 13.6233 - val_acc: 0.9738 - val_mean_iou: 0.7523\n",
            "Epoch 13/25\n",
            "722/722 [==============================] - 1079s 1s/step - loss: 12.2995 - acc: 0.9738 - mean_iou: 0.7426 - val_loss: 12.8119 - val_acc: 0.9746 - val_mean_iou: 0.7612\n",
            "Epoch 14/25\n",
            "722/722 [==============================] - 1104s 2s/step - loss: 12.2750 - acc: 0.9741 - mean_iou: 0.7415 - val_loss: 12.4996 - val_acc: 0.9718 - val_mean_iou: 0.7416\n",
            "Epoch 15/25\n",
            "722/722 [==============================] - 1101s 2s/step - loss: 12.1129 - acc: 0.9744 - mean_iou: 0.7469 - val_loss: 12.7179 - val_acc: 0.9674 - val_mean_iou: 0.7131\n",
            "Epoch 16/25\n",
            "722/722 [==============================] - 1115s 2s/step - loss: 12.0735 - acc: 0.9746 - mean_iou: 0.7492 - val_loss: 12.5778 - val_acc: 0.9726 - val_mean_iou: 0.7407\n",
            "Epoch 17/25\n",
            "722/722 [==============================] - 1093s 2s/step - loss: 12.0166 - acc: 0.9748 - mean_iou: 0.7497 - val_loss: 12.4752 - val_acc: 0.9686 - val_mean_iou: 0.7215\n",
            "Epoch 18/25\n",
            "722/722 [==============================] - 1104s 2s/step - loss: 11.8775 - acc: 0.9753 - mean_iou: 0.7551 - val_loss: 12.4944 - val_acc: 0.9740 - val_mean_iou: 0.7543\n",
            "Epoch 19/25\n",
            "722/722 [==============================] - 1101s 2s/step - loss: 11.8311 - acc: 0.9757 - mean_iou: 0.7580 - val_loss: 12.3741 - val_acc: 0.9716 - val_mean_iou: 0.7274\n",
            "Epoch 20/25\n",
            "722/722 [==============================] - 1096s 2s/step - loss: 11.7477 - acc: 0.9761 - mean_iou: 0.7609 - val_loss: 12.4653 - val_acc: 0.9733 - val_mean_iou: 0.7540\n",
            "Epoch 21/25\n",
            "722/722 [==============================] - 1085s 2s/step - loss: 11.6307 - acc: 0.9765 - mean_iou: 0.7672 - val_loss: 12.3395 - val_acc: 0.9725 - val_mean_iou: 0.7462\n",
            "Epoch 22/25\n",
            "722/722 [==============================] - 1098s 2s/step - loss: 11.5655 - acc: 0.9768 - mean_iou: 0.7683 - val_loss: 12.4025 - val_acc: 0.9725 - val_mean_iou: 0.7376\n",
            "Epoch 23/25\n",
            "722/722 [==============================] - 1101s 2s/step - loss: 11.4877 - acc: 0.9770 - mean_iou: 0.7695 - val_loss: 12.3682 - val_acc: 0.9727 - val_mean_iou: 0.7436\n",
            "Epoch 24/25\n",
            "722/722 [==============================] - 1110s 2s/step - loss: 11.4247 - acc: 0.9772 - mean_iou: 0.7727 - val_loss: 12.3527 - val_acc: 0.9728 - val_mean_iou: 0.7450\n",
            "Epoch 25/25\n",
            "722/722 [==============================] - 1103s 2s/step - loss: 11.3925 - acc: 0.9771 - mean_iou: 0.7746 - val_loss: 12.3631 - val_acc: 0.9729 - val_mean_iou: 0.7441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e2ntuKBgGiTJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(12,4))\n",
        "# plt.subplot(131)\n",
        "# plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
        "# plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Valid loss\")\n",
        "# plt.legend()\n",
        "# plt.subplot(132)\n",
        "# plt.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n",
        "# plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Valid accuracy\")\n",
        "# plt.legend()\n",
        "# plt.subplot(133)\n",
        "# plt.plot(history.epoch, history.history[\"mean_iou\"], label=\"Train iou\")\n",
        "# plt.plot(history.epoch, history.history[\"val_mean_iou\"], label=\"Valid iou\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1hA7Q6K1Gkwj",
        "colab_type": "code",
        "outputId": "b23df59e-2e58-4260-bfd2-ec20118f95bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# load and shuffle filenames\n",
        "folder = './stage_1_test_images.zip_files'\n",
        "test_filenames = os.listdir(folder)\n",
        "print('n test samples:', len(test_filenames))\n",
        "\n",
        "# create test generator with predict flag set to True\n",
        "test_gen = generator(folder, test_filenames, None, batch_size=32, image_size=256, shuffle=False, predict=True)\n",
        "\n",
        "# create submission dictionary\n",
        "submission_dict = {}\n",
        "# loop through testset\n",
        "for imgs, filenames in test_gen:\n",
        "    # predict batch of images\n",
        "    preds = model.predict(imgs)\n",
        "    # loop through batch\n",
        "    for pred, filename in zip(preds, filenames):\n",
        "        # resize predicted mask\n",
        "        pred = resize(pred, (1024, 1024), mode='reflect')\n",
        "        # threshold predicted mask\n",
        "        comp = pred[:, :, 0] > 0.5\n",
        "        # apply connected components\n",
        "        comp = measure.label(comp)\n",
        "        # apply bounding boxes\n",
        "        predictionString = ''\n",
        "        for region in measure.regionprops(comp):\n",
        "            # retrieve x, y, height and width\n",
        "            y, x, y2, x2 = region.bbox\n",
        "            height = y2 - y\n",
        "            width = x2 - x\n",
        "            # proxy for confidence score\n",
        "            conf = np.mean(pred[y:y+height, x:x+width])\n",
        "            # add to predictionString\n",
        "            predictionString += str(conf) + ' ' + str(x) + ' ' + str(y) + ' ' + str(width) + ' ' + str(height) + ' '\n",
        "        # add filename and predictionString to dictionary\n",
        "        filename = filename.split('.')[0]\n",
        "        submission_dict[filename] = predictionString\n",
        "    # stop if we've got them all\n",
        "    if len(submission_dict) >= len(test_filenames):\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n test samples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i1bUzxikGrk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save dictionary as csv file\n",
        "sub = pd.DataFrame.from_dict(submission_dict,orient='index')\n",
        "sub.index.names = ['patientId']\n",
        "sub.columns = ['PredictionString']\n",
        "sub.to_csv('1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4y1g-9TRvGC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ]
    }
  ]
}